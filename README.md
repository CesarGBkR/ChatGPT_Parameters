# ChatGPT Parameters

<p align="center">
<img src="https://images.weserv.nl/?url=cdn.oaistatic.com/_next/static/media/apple-touch-icon.59f2e898.png?v=4&h=200&w=200&fit=cover&mask=circle" width="200" height="200">
</p>

# Parametros para afinacion de prompts en ChatGPT
Dentro de los llamados *promts* (Texto usado para generar respuestas dentro de ChatGPT) existe una serie de parametros que ayudan a generar una respuesta mas especifica segun los intereses

## Parametros identificados

| Nombre                              | Descripcion                                                                                             | Rango                                                                                                                                     |
| ----------------------------------- | ------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------- |
| **Temperatura**                     | Controla la aleatoriedad de las respuestas generadas                                                    | Entre 0.1 y 2.0. Valores más bajos producen respuestas más conservadoras, mientras que valores más altos generan respuestas más diversas. |
| **Top-k**                           | Limita la selección de tokens durante la generación de texto a los k tokens más probables en cada paso. | Depende del tamaño del vocabulario, pero generalmente entre 1 y el tamaño del vocabulario.                                                |
| **Top-p (Nucleus Sampling)**       | Controla la cantidad acumulativa de probabilidad considerada durante la selección de tokens.            | Normalmente entre 0.1 y 1.0. Valores más bajos generan respuestas más restrictivas.                                                       |
| **Max tokens**                     | Establece el límite máximo de tokens que se generarán en una única respuesta.                           | Dependiendo del modelo, puede variar desde unos pocos cientos hasta varios miles.                                                         |
| **Presencia de tokens de control** | Modifica el comportamiento de la generación de texto mediante tokens especiales.                        | Valores: Sí o No.                                                                                                                         |
| **Frecuencia de muestreo**         | Controla la frecuencia con la que se generan muestras durante el proceso de generación.                 | Normalmente entre 0.1 y 1.0. Valores más altos aumentan la diversidad pero pueden aumentar el tiempo de generación.                       |
| **Atenuación de la repetición**    | Controla la propensión del modelo a repetir frases o patrones en la generación de texto.                | Normalmente entre 0.0 y 1.0. Valores más altos reducen la repetición.                                                                     |
| **Atenuación de la penalización de largo plazo**                    | Ayuda a controlar la tendencia del modelo a olvidar información relevante a medida que avanza en la generación de texto.                                              | Normalmente entre 0.0 y 1.0. Valores más altos reducen la tendencia al olvido.                                                                       |
| **Fuerza de la penalización de largo plazo**                        | Define la intensidad con la que se penaliza la repetición de información a lo largo de la generación de texto.                                                        | Normalmente entre 0.0 y 2.0. Valores más altos aumentan la penalización por repetición.                                                              |
| **Tamaño del batch**                                                | Controla la cantidad de ejemplos que se procesan simultáneamente durante la generación de texto.                                                                      | Depende de la capacidad del sistema y del modelo, pero generalmente entre 1 y algunos cientos.                                                       |
| **Longitud mínima y máxima del texto generado**                     | Establece los límites mínimo y máximo para la longitud de las respuestas generadas.                                                                                   | Normalmente entre 1 y un número determinado de tokens para la longitud mínima, y entre la longitud mínima y un número mayor para la longitud máxima. |
| **Tasa de aprendizaje (en modelos entrenables)**                    | Controla la velocidad a la que el modelo ajusta sus parámetros durante el entrenamiento.                                                                              | Depende del algoritmo de optimización utilizado y del problema específico, pero generalmente en el rango de 0.0001 a 0.1.                            |
| **Número de iteraciones de entrenamiento (en modelos entrenables)** | Determina cuántas veces se repite el proceso de entrenamiento sobre el conjunto de datos.                                                                             | Depende del modelo y del tamaño del conjunto de datos, pero generalmente entre 1 y varios cientos o miles.                                           |
| **Arquitectura del modelo**                                         | Algunos modelos pueden permitir la selección de diferentes arquitecturas, como GPT, BERT, Transformer, etc.                                                           | Depende de las opciones proporcionadas por el modelo específico.                                                                                     |
| **Probabilidad de dropout**                                         | Controla la probabilidad de ignorar aleatoriamente ciertas unidades durante el entrenamiento para evitar el sobreajuste.                                              | Normalmente entre 0.0 y 1.0.                                                                                                                         |
| **Función de activación**                                           | Especifica la función matemática utilizada para introducir no linealidad en la red neuronal.                                                                          | Pueden incluir funciones como ReLU, sigmoid, tanh, entre otras.                                                                                      |
| **Capas de atención**                                               | Determina la cantidad de capas de atención utilizadas en la arquitectura del modelo, lo que puede afectar la capacidad de atención y la interpretación de la entrada. | Normalmente entre 1 y varios cientos, dependiendo del modelo.                                                                                        |
| **Dimensiones del embedding**                                       | Define el tamaño del espacio de representación para las palabras en el vocabulario.                                                                                   | Depende del tamaño del vocabulario y de la complejidad del problema, pero generalmente entre 50 y 1000.                                       |
| **Número de cabezas de atención**                                   | Controla la cantidad de "cabezas" de atención utilizadas en la arquitectura del modelo de atención.                                                                   | Normalmente entre 1 y varios cientos.                                                                                                                |
| **Función de pérdida**                                              | Especifica la función utilizada para calcular la pérdida durante el entrenamiento del modelo.                                                                         | Pueden incluir funciones como la entropía cruzada, la pérdida cuadrática, etc.                                                                       |
| **Tamaño del contexto**                                             | Define la cantidad de tokens anteriores que se tienen en cuenta al generar cada token siguiente en el texto.                                                          | Depende del modelo y del contexto de la aplicación, pero puede variar desde unos pocos tokens hasta varios cientos.                                  |
| **Regularización**                                                  | Controla la magnitud de los pesos del modelo para evitar el sobreajuste durante el entrenamiento.                                                                     | Pueden incluir términos de regularización L1, L2, dropout, entre otros.                                                                              |
| **Decaimiento de la tasa de aprendizaje**                           | Especifica cómo cambia la tasa de aprendizaje a lo largo del tiempo durante el entrenamiento.                                                                         | Pueden incluir tasas de decaimiento lineales, exponenciales, polinomiales, entre otras.                                                              |
| **Inicialización de pesos**                                         | Define cómo se inicializan los pesos del modelo al comienzo del entrenamiento.                                                                                        | Pueden incluir inicialización aleatoria, inicialización uniforme, inicialización Glorot, entre otras.                                                |
| **Función de optimización**                                         | Especifica el algoritmo utilizado para optimizar los pesos del modelo durante el entrenamiento.                                                                       | Pueden incluir algoritmos como el descenso de gradiente estocástico (SGD), Adam, RMSprop, entre otros.                                               |


*Toda la informacion anterior fue generada por el mismo ChatGPT*
